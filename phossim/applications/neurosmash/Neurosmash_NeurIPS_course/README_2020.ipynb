{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SOW-MKI49-2020-SEM1-V: NeurIPS**\n",
    "#### Project: Neurosmash\n",
    "\n",
    "This is the info document on the Neurosmash environment that you will be using for your Final Assignment. It contains background info and skeleton code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project\n",
    "\n",
    "During the 2nd period, you will be working exclusively on this final group project in the practicals. You are expected to form groups of 4-5 people. The goal is to take what has been discussed in class and what you have already worked on in the earlier practicals, and apply them on a RL problem in a novel environment. This  project will constitute 25% of your final grade.\n",
    "\n",
    "Your project grade will be based on the following components:\n",
    "- Online demonstration\n",
    "- Source code\n",
    "- Written report (a 4-page report in NeurIPS workshop paper format: https://www.overleaf.com/latex/templates/neurips-2020/mnshsmqkjsqz)\n",
    "\n",
    "These components will be evaluated based on performance, creativity, elegance, rigor and plausibility.\n",
    "\n",
    "While you can use the material from earlier practicals (e.g., REINFORCE, DQN, etc.) as a boilerplate, you are also free to take any other approach be it imitation learning or world models for your project.\n",
    "\n",
    "As a deep learning library, use of mxnet is preferred. Still, you are free to use whatever you want.\n",
    "\n",
    "In addition to the practical sessions, we will provide additional support in the coming weeks. You can email any of us to set up an appointment for discussing your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "Briefly, there are two agents: Red and Blue. Red is controlled by you. Blue is controlled by the environment \"AI\".* Both agents always run forward with a speed of 3.5 m/s*. If one of them gets within the reach of the other (a frontal sphere with 0.5 m radius), it gets pushed away automatically with a speed of 3.5 m/s. The only thing that the agents can do is to turn left or right with an angular speed of 180 degrees/s. This means that there are three possible discrete actions that your agent can take every step: Turn nowhere, turn left and turn right. For convenience, there is also a fourth built-in action which turns left or right with uniform probability. An episode begins when you reset the environment and ends when one of the agents fall off the platform. At the end of the episode, the winning agent gets a reward of 10 while the other gets nothing. Therefore, your goal is to train an agent who can maximize its reward by pushing the other agent off the platform or making it fall off the platform by itself.\n",
    "\n",
    "* Note that all times are simulation time. That is, 0.02 s per step when timescale is set to one.\n",
    "\n",
    "* Basically, Blue is artificial but not really intelligent. What it does is that every 0.5 s, it updates its destination to the current position of Red plus some random variation (a surrounding circle with a radius of 1.75 m) and smoothly turns to that position.\n",
    "\n",
    "Note to macOS users: You should first make the environment executable* in the terminal and run it from the context menu (i.e., not by double clicking)\n",
    "\n",
    "* chmod -R +x [Path of Mac.app (which is in the .zip file)]/Contents/MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton code\n",
    "\n",
    "- You should first add the Neurosmash file to your working directory or Python path.\n",
    "- Next you should start the Neurosmash app \n",
    "- Make sure to set the right values in the Ip, Port, Size and Timescale fields (see below). These must correspond to the values you specify in the python script\n",
    "- Start the server by pressing the play button\n",
    "- The fastest simulations can be obtained by turning off rendering (x button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Neurosmash\n",
    "\n",
    "# These are the default environment arguments. They must be the same as the values that are set in the environment GUI.\n",
    "ip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to (127.0.0.1 by default)\n",
    "port       = 13000       # Port number that the TCP/IP interface listens to (13000 by default)\n",
    "\n",
    "# This is the size of the texture that the environment is rendered.\n",
    "# This is set to 784 by default, which will result in a crisp image but slow speed.\n",
    "# You can change the size to a value that works well for your environment but should not go too low.\n",
    "size       = 96\n",
    "\n",
    "# This is the simulation speed of the environment. This is set to 1 by default.\n",
    "# Setting it to n will make the simulation n times faster.\n",
    "# In other words, less (if n < 1) or more (if n > 1) simulation time will pass per step.\n",
    "# You might want to increase this value to around 10 if you cannot train your models fast enough\n",
    "# so that they can sample more states in a shorter number of steps at the expense of precision.\n",
    "timescale  = 5\n",
    "\n",
    "# This is an example agent.\n",
    "# It has a step function, which gets reward/state as arguments and returns an action.\n",
    "# Right now, it always outputs a random action (3) regardless of reward/state.\n",
    "# The real agent should output one of the following three actions:\n",
    "# none (0), left (1) and right (2)\n",
    "agent = Neurosmash.Agent() \n",
    "\n",
    "# This is the main environment.\n",
    "# It has a reset function, which is used to reset the environment before episodes.\n",
    "# It also has a step function, which is used to which steps one time point\n",
    "# It gets an action (as defined above) as input and outputs the following:\n",
    "# end (true if the episode has ended, false otherwise)\n",
    "# reward (10 if won, 0 otherwise)\n",
    "# state (flattened size x size x 3 vector of pixel values)\n",
    "# The state can be converted into an image as follows:\n",
    "# image = np.array(state, \"uint8\").reshape(size, size, 3)\n",
    "# You can also use to Neurosmash.Environment.state2image(state) function which returns\n",
    "# the state as a PIL image\n",
    "environment = Neurosmash.Environment(ip, port, size, timescale) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following steps through an entire episode from start to finish with random actions (by default)\n",
    "\n",
    "end, reward, state = environment.reset()\n",
    "\n",
    "while (end == 0):\n",
    "    action = agent.step(end, reward, state)\n",
    "    end, reward, state = environment.step(action)\n",
    "\n",
    "# Let's run it a few more steps so that the things have time to settle down\n",
    "\n",
    "for i in range(100):\n",
    "    action = agent.step(end, reward, state)\n",
    "    end, reward, state = environment.step(action)\n",
    "\n",
    "environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also do it step by step while displaying the state\n",
    "\n",
    "end, reward, state = environment.reset()\n",
    "\n",
    "environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = agent.step(end, reward, state)\n",
    "end, reward, state = environment.step(action)\n",
    "\n",
    "environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = agent.step(end, reward, state)\n",
    "end, reward, state = environment.step(action)\n",
    "\n",
    "environment.state2image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('dev': conda)",
   "display_name": "Python 3.7.9 64-bit ('dev': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2d4e5c8e62def408b095e01fcaa956bce6b38736eb6ff7b76beb42c88b938b98"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}